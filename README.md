# SCIVER: A Benchmark for Multimodal Scientific Claim Verification

<p align="center">
  <a href="https://github.com/QDRhhhh/SciVer">ğŸŒ Github</a> â€¢
  <a href="">ğŸ“– Paper</a> â€¢
  <a href="https://huggingface.co/datasets/chengyewang/SciVer">ğŸ¤— Data</a>
</p>

## ğŸ“° News
- [May 15, 2025] SciVer has been accepted by ACL 2025 Main!

## ğŸ‘‹ Overview

![image-20250603111710602](./README.assets/image-20250603111710602.png)

**SCIVER** is the first benchmark specifically designed to evaluate the ability of foundation models to verify scientific claims across **text**, **charts**, and **tables**. It challenges models to reason over complex, multimodal contexts with **fine-grained entailment labels** and **expert-annotated rationales**.

> ğŸ“Œ â€œCan Multimodal Foundation Models Reason Over Scientific Claims with Text, Tables, and Charts?â€

------

## ğŸŒŸ Highlights

- ğŸ§ª **3,000 expert-annotated examples** from **1113 scientific papers**
- ğŸ§  Four core **reasoning subsets**:
  - Direct
  - Parallel
  - Sequential
  - Analytical
- ğŸ“š Context includes **text paragraphs, multiple tables, and charts**
- ğŸ” Labels: `Entailed`, `Refuted`
- ğŸ“ˆ Evaluated across **21 leading foundation models**, including GPT-4o, Gemini, Claude 3.5, Qwen2.5-VL, LLaMA-3.2-Vision, etc.
- âš–ï¸ Includes **step-by-step rationale** and **automated accuracy evaluation**

------

## ğŸ§© Benchmark Structure

Each SCIVER sample includes:

- A **claim** grounded in multimodal scientific context
- **Contextual inputs**: text, tables (as images), charts (as images)
- A **gold entailment label** (entailed / refuted)
- **Supporting evidence** and a **reasoning rationale**

### ğŸ§  Subsets by Reasoning Type

1. **Direct Reasoning** â€“ extract simple facts
2. **Parallel Reasoning** â€“ synthesize info from multiple sources
3. **Sequential Reasoning** â€“ perform step-by-step inference
4. **Analytical Reasoning** â€“ apply domain expertise and logic

------

## ğŸ“Š Model Evaluation

We evaluate 21 models using Chain-of-Thought prompting.

| Model            | Accuracy  |
| ---------------- | --------- |
| ğŸ§‘â€ğŸ”¬Human Expert   | **93.8%** |
| o4-mini (OpenAI) | 77.7%     |
| GPT-4o           | 70.9%     |
| Qwen2.5-VL-72B   | 69.4%     |
| InternVL3-38B    | 62.5%     |

> Text-only versions of models drop 35â€“53% in accuracy â€” showing **multimodal context is essential**.

------

## ğŸ› ï¸ Quickstart

### ğŸ” Step 0: Installation

```bash
git clone https://github.com/QDRhhhh/SciVer.git
cd SciVer
conda create --name sciver python=3.10
conda activate sciver
pip install -r requirements.txt
```

### ğŸ” Step 1: Download Dataset from huggingface

```bash
git lfs install
git clone https://huggingface.co/datasets/chengyewang/SciVer
```

### ğŸ” Step 2: Run Model Inference

```bash
bash scripts/vllm_large.sh
```

This will generate model responses and save them to:

```
./outputs/
```

### âœ… Step 3: Evaluate Model Accuracy

```bash
python acc_evaluation.py
```

The processed results and accuracy scores will be saved to:

```
./processed_outputs/
```

------

## ğŸ¤ Contributing

We welcome contributions for:

- ğŸ§¬ Domain extension (e.g., biology, medicine)
- ğŸ”§ Additional model adapters
- ğŸ“ˆ New evaluation metrics and visualization tools